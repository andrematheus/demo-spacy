{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b24295bf5bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Here's how to download a large model for English:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#  python -m spacy download en_core_web_lg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mdocument_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'happy glad cheddar munster'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andre.matheus/.virtualenvs/demospacy/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andre.matheus/.virtualenvs/demospacy/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# Set up functions to help produce human-friendly printing.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "def skip_and_print(*args):\n",
    "    \"\"\" Act like print(), but skip a line before printing. \"\"\"\n",
    "    print('\\n' + str(args[0]), *args[1:])\n",
    "\n",
    "# The next definitions work together to help print tables.\n",
    "\n",
    "col_widths = []\n",
    "\n",
    "def row(*values):\n",
    "    \"\"\" Return a str with `values` spaced by `col_widths`. \"\"\"\n",
    "    fmt = ' '.join('%%%ds' % width for width in col_widths)\n",
    "    return fmt % tuple(values)\n",
    "\n",
    "def hr():\n",
    "    \"\"\" Return a horizontal rule str using `col_widths`. \"\"\"\n",
    "    size = sum(map(abs, col_widths)) + len(col_widths) - 1\n",
    "    return '~' * size\n",
    "\n",
    "\n",
    "# Load a language model and parse a document.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# spacy's default English language model doesn't have word\n",
    "# vector data, so we must use a larger variant to use vectors.\n",
    "# Here's how to download a large model for English:\n",
    "#  python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "document_string = 'happy glad cheddar munster'\n",
    "\n",
    "skip_and_print('Working with string: \"%s\"' % document_string)\n",
    "doc = nlp(document_string)\n",
    "\n",
    "\n",
    "# Retrieve word vectors for each token.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "skip_and_print('Word vectors for each token:\\n')\n",
    "\n",
    "col_widths = [7, 1, -30]\n",
    "print(row('Word', '|', 'Vector'))\n",
    "print(hr())\n",
    "\n",
    "for token in doc:\n",
    "    v = token.vector\n",
    "    v_str = ('%5.2f ' * 4) % tuple(v[:4])\n",
    "    print(row(token, '|', '(' + v_str + '...)'))\n",
    "\n",
    "# Word vectors for each token:\n",
    "#\n",
    "#    Word | Vector\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#   happy | ( 0.04  0.41 -0.52 -0.07 ...)\n",
    "#    glad | ( 0.07  0.25 -0.53 -0.03 ...)\n",
    "# cheddar | (-0.63  0.53  0.23 -0.16 ...)\n",
    "# munster | (-0.15  0.76  0.48  0.23 ...)\n",
    "\n",
    "\n",
    "# Compute word similarities using vector data.\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "skip_and_print('Similarity matrix for the above words:\\n')\n",
    "\n",
    "col_widths = [7] * 5  # Our longest word has length 7.\n",
    "print(row('', *doc))\n",
    "print(hr())\n",
    "\n",
    "for token1 in doc:\n",
    "    values = [token1]\n",
    "    for token2 in doc:\n",
    "        values.append('%5.2f' % token1.similarity(token2))\n",
    "    print(row(*values))\n",
    "\n",
    "# Similarity matrix for the above words:\n",
    "# \n",
    "#           happy    glad cheddar munster\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#   happy    1.00    0.77    0.06   -0.01\n",
    "#    glad    0.77    1.00    0.07    0.00\n",
    "# cheddar    0.06    0.07    1.00    0.21\n",
    "# munster   -0.01    0.00    0.21    1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
